import os
import json
import streamlit as st
from modules.pdf_text_extractor import extract_text_from_pdf
from modules.text_cleaning import clean_text
from modules.pdf_feature_extractor import extract_entities
from modules.text_chunker import smart_chunk_text, save_chunks_to_json
from modules.vector_database import save_to_faiss, search_faiss

# ğŸ“Œ Directories
UPLOAD_DIR = "uploaded_pdfs"
EXTRACTED_DIR = os.path.join(UPLOAD_DIR, "extracted")
os.makedirs(EXTRACTED_DIR, exist_ok=True)

# ğŸ¨ Streamlit UI
st.set_page_config(page_title="Crypto Due Diligence", layout="wide")

st.title("ğŸ“„ Crypto Due Diligence AI")
st.markdown("ğŸ” Upload a **PDF** document to analyze its financial, crypto, and legal risks.")

# ğŸ“Œ File Uploader
uploaded_file = st.file_uploader("Upload your PDF", type=["pdf"])

if uploaded_file is not None:
    pdf_filename = uploaded_file.name
    pdf_path = os.path.join(UPLOAD_DIR, pdf_filename)

    # Save uploaded file
    with open(pdf_path, "wb") as f:
        f.write(uploaded_file.getbuffer())

    st.success(f"âœ… File {pdf_filename} uploaded successfully!")

    # ğŸ“Œ Step 1: Extract Text
    st.write("ğŸ“„ Extracting text...")
    extracted_text = extract_text_from_pdf(pdf_path)

    # Save extracted text
    text_file = os.path.join(EXTRACTED_DIR, f"{pdf_filename}.txt")
    with open(text_file, "w", encoding="utf-8") as f:
        f.write(extracted_text)

    st.success("âœ… Text extracted successfully!")

    # ğŸ“Œ Step 2: Clean Text
    st.write("ğŸ§¹ Cleaning text...")
    cleaned_text = clean_text(extracted_text)
    st.success("âœ… Text cleaned!")

    # ğŸ“Œ Step 3: Extract Entities
    st.write("ğŸ“Š Extracting entities (financial, crypto, legal)...")
    analysis_results = extract_entities(cleaned_text)

    # Save analysis results
    analysis_file = os.path.join(EXTRACTED_DIR, f"{pdf_filename}_analysis.json")
    with open(analysis_file, "w", encoding="utf-8") as json_file:
        json.dump(analysis_results, json_file, indent=4)

    st.success("âœ… Entity extraction completed!")

    # ğŸ“Œ Display Extracted Entities
    st.subheader("ğŸ“Œ Extracted Entities")
    st.json(analysis_results)

    # ğŸ“Œ Step 4: Chunking & Embedding
    st.write("ğŸ”¹ Chunking text & generating embeddings with Nomic...")
    chunks = smart_chunk_text(cleaned_text)

    # Save chunks
    chunk_file = os.path.join(EXTRACTED_DIR, f"{pdf_filename}_chunks.json")
    save_chunks_to_json(chunks, chunk_file)

    # Save embeddings to FAISS
    index_file = os.path.join(EXTRACTED_DIR, f"{pdf_filename}_embeddings.index")
    save_to_faiss(chunks, index_path=index_file)

    st.success("âœ… Chunking & Embeddings saved!")

    # ğŸ“Œ Enable Q&A
    st.subheader("ğŸ§ Ask Questions About the Document")
    user_query = st.text_input("Type your question:")

    if st.button("ğŸ” Search"):
        if user_query:
            results = search_faiss(user_query, k=5, index_path=index_file)

            # âœ… **Ensure Correct Formatting for Display**
            st.subheader("ğŸ“Œ Top Answers from Document")
            if isinstance(results, str):
                st.markdown(f"**Answer:** {results}")  # âœ… Ensure correct response format
            elif isinstance(results, list) and results:
                formatted_text = " ".join(results)  # âœ… Join text properly for readability
                st.markdown(f"**Answer:** {formatted_text}")
            else:
                st.warning("âš ï¸ No relevant answers found. Try another question.")
        else:
            st.warning("âš ï¸ Please enter a question.")
